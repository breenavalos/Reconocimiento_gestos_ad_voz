{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11739a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install flask-socketio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc574e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install waitress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87249c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gevent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b56404",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gevent-websocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8559b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V1\n",
    "#Import necessary libraries\n",
    "from flask import Flask, render_template, Response\n",
    "\n",
    "import cv2\n",
    "#Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "def gen_frames():  \n",
    "    while True:\n",
    "        success, frame = camera.read()  # read the camera frame\n",
    "        if not success:\n",
    "            break\n",
    "        else:\n",
    "            ret, buffer = cv2.imencode('.jpg', frame)\n",
    "            frame = buffer.tobytes()\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')  # concat frame one by one and show result\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "     return render_template(\"Index.html\")\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BAVALOS\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\BAVALOS\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\BAVALOS\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#V2\n",
    "#Import necessary libraries\n",
    "from flask import Flask, render_template, Response\n",
    "from flask_socketio import SocketIO, emit\n",
    "\n",
    "import cv2\n",
    "#Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "app.config['SECRET_KEY'] = 'mysecretkey'\n",
    "socketio = SocketIO(app)\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "@socketio.on('transcription')\n",
    "def handle_transcription(data):\n",
    "    print(\"Hola\")\n",
    "    # Realizar la transcripción del audio (aquí puedes utilizar el servicio o biblioteca de tu elección)\n",
    "    transcription = 'Transcripción del audio'\n",
    "\n",
    "    # Enviar la transcripción al cliente\n",
    "    socketio.emit('transcription', {'text': transcription})\n",
    "    \n",
    "\n",
    "def gen_frames():  \n",
    "    while True:\n",
    "        success, frame = camera.read()  # read the camera frame\n",
    "        if not success:\n",
    "            break\n",
    "        else:\n",
    "            ret, buffer = cv2.imencode('.jpg', frame)\n",
    "            frame = buffer.tobytes()\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')  # concat frame one by one and show result\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "     return render_template(\"Index.html\")\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #app.run(debug=False)\n",
    "    socketio.run(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cf48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb111b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportación del modelo de reconocimiento facial\n",
    "#--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bf8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEPENDENCIAS NECESARIO.\n",
    "import os\n",
    "import wget\n",
    "import object_detection\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' \n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    " }\n",
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}\n",
    "labels = [{'name':'HOLA', 'id':1}, {'name':'CHAU', 'id':2}, {'name':'PERMISO', 'id':3}, {'name':'AMOR', 'id':4}, {'name':'GRACIAS', 'id':5}]\n",
    "\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')\n",
    "        \n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-3')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2344724",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48657bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V2\n",
    "#Import necessary libraries\n",
    "from flask import Flask, render_template, Response\n",
    "from flask_socketio import SocketIO, emit\n",
    "\n",
    "import cv2\n",
    "#Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "app.config['SECRET_KEY'] = 'mysecretkey'\n",
    "socketio = SocketIO(app)\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "@socketio.on('transcription')\n",
    "def handle_transcription(data):\n",
    "    print(\"Hola\")\n",
    "    # Realizar la transcripción del audio (aquí puedes utilizar el servicio o biblioteca de tu elección)\n",
    "    transcription = 'Transcripción del audio'\n",
    "\n",
    "    # Enviar la transcripción al cliente\n",
    "    socketio.emit('transcription', {'text': transcription})\n",
    "    \n",
    "\n",
    "def gen_frames():  \n",
    "    while True:\n",
    "        success, frame = camera.read()  # read the camera frame\n",
    "        if not success:\n",
    "            break\n",
    "        else:\n",
    "            ret, buffer = cv2.imencode('.jpg', frame)\n",
    "            frame = buffer.tobytes()\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')  # concat frame one by one and show result\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "     return render_template(\"Index.html\")\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #app.run(debug=False)\n",
    "    socketio.run(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7407380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
